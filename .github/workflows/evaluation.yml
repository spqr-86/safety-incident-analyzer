name: RAG System Evaluation

on:
  # –ó–∞–ø—É—Å–∫ –ø—Ä–∏ push –≤ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–µ—Ç–∫–∏
  push:
    branches:
      - main
      - develop
      - claude/**
    paths:
      - 'src/**'
      - 'agents/**'
      - 'config/**'
      - 'tests/dataset*.csv'
      - 'eval/**'
      - '.github/workflows/evaluation.yml'

  # –ó–∞–ø—É—Å–∫ –ø—Ä–∏ pull request
  pull_request:
    branches:
      - main
      - develop
    paths:
      - 'src/**'
      - 'agents/**'
      - 'config/**'

  # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫
  workflow_dispatch:
    inputs:
      limit:
        description: 'Limit number of questions to evaluate'
        required: false
        default: '10'
      full_evaluation:
        description: 'Run full evaluation (all questions)'
        required: false
        type: boolean
        default: false

  # –ü–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é (–µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ –≤ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –≤ 00:00 UTC)
  schedule:
    - cron: '0 0 * * 0'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # –ü–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: üì¶ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: üîç Check dataset exists
        run: |
          if [ ! -f "tests/dataset.csv" ]; then
            echo "‚ùå Dataset not found: tests/dataset.csv"
            exit 1
          fi
          echo "‚úÖ Dataset found"
          wc -l tests/dataset.csv

      - name: üìä Run evaluation
        id: eval
        env:
          GIGACHAT_CREDENTIALS: ${{ secrets.GIGACHAT_CREDENTIALS }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_TRACING_V2: true
          LANGSMITH_PROJECT: safety-incident-analyzer-ci
        run: |
          # –û–ø—Ä–µ–¥–µ–ª—è–µ–º limit
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            if [ "${{ inputs.full_evaluation }}" = "true" ]; then
              LIMIT=""
            else
              LIMIT="--limit ${{ inputs.limit }}"
            fi
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            # Weekly scheduled run - –ø–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            LIMIT=""
          else
            # PR –∏–ª–∏ push - –±—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ (5 –≤–æ–ø—Ä–æ—Å–æ–≤)
            LIMIT="--limit 5"
          fi

          echo "üöÄ Running evaluation with params: $LIMIT"
          python eval/run_full_evaluation.py $LIMIT \
            --output benchmarks/results_history.jsonl \
            2>&1 | tee eval_output.log

          # –°–æ—Ö—Ä–∞–Ω—è–µ–º exit code
          echo "eval_exit_code=$?" >> $GITHUB_OUTPUT

      - name: üìà Compare with baseline
        id: compare
        if: always()
        run: |
          if [ -f "benchmarks/baseline.json" ] && [ -f "benchmarks/results_history.jsonl" ]; then
            echo "üìä Comparing metrics with baseline..."
            python scripts/compare_with_baseline.py \
              --baseline benchmarks/baseline.json \
              --output benchmarks/comparison_result.json \
              2>&1 | tee comparison_output.log || true

            echo "comparison_done=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è Baseline or results not found, skipping comparison"
            echo "comparison_done=false" >> $GITHUB_OUTPUT
          fi

      - name: üìù Generate summary
        if: always()
        run: |
          echo "## üìä Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "eval_output.log" ]; then
            echo "### Evaluation Output" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -n 50 eval_output.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f "comparison_output.log" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Comparison with Baseline" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat comparison_output.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: üí¨ Comment on PR
        if: github.event_name == 'pull_request' && steps.compare.outputs.comparison_done == 'true'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            let comment = '## üìä Evaluation Results\n\n';

            if (fs.existsSync('comparison_output.log')) {
              const comparison = fs.readFileSync('comparison_output.log', 'utf8');
              comment += '### Metrics Comparison\n```\n' + comparison + '\n```\n\n';
            }

            if (fs.existsSync('benchmarks/comparison_result.json')) {
              const result = JSON.parse(fs.readFileSync('benchmarks/comparison_result.json', 'utf8'));

              if (result.regressions && result.regressions.length > 0) {
                comment += '### ‚ö†Ô∏è Warning: Metric Regressions Detected\n\n';
                result.regressions.forEach(r => {
                  comment += `- **${r.metric}**: ${r.baseline.toFixed(3)} ‚Üí ${r.current.toFixed(3)} (${r.change_pct.toFixed(1)}%)\n`;
                });
                comment += '\n';
              }

              if (result.improvements && result.improvements.length > 0) {
                comment += '### ‚úÖ Improvements\n\n';
                result.improvements.forEach(r => {
                  comment += `- **${r.metric}**: ${r.baseline.toFixed(3)} ‚Üí ${r.current.toFixed(3)} (+${r.change_pct.toFixed(1)}%)\n`;
                });
              }
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: üì§ Upload artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-results
          path: |
            benchmarks/results_history.jsonl
            benchmarks/comparison_result.json
            eval_output.log
            comparison_output.log
          retention-days: 30

      - name: ‚ùå Fail on regressions
        if: github.event_name == 'pull_request'
        run: |
          if [ -f "benchmarks/comparison_result.json" ]; then
            REGRESSIONS=$(python -c "import json; r=json.load(open('benchmarks/comparison_result.json')); print(r['summary']['regressions'])")
            if [ "$REGRESSIONS" -gt "0" ]; then
              echo "‚ùå Evaluation failed: $REGRESSIONS metric regressions detected"
              echo "Review the comparison output above for details"
              exit 1
            fi
          fi
          echo "‚úÖ No regressions detected"

  # –û—Ç–¥–µ–ª—å–Ω—ã–π job –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ drift (—Ç–æ–ª—å–∫–æ –¥–ª—è scheduled runs)
  drift-detection:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    needs: evaluate
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v3

      - name: üìß Send alert on drift
        if: needs.evaluate.result == 'failure'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.ALERT_EMAIL_USERNAME }}
          password: ${{ secrets.ALERT_EMAIL_PASSWORD }}
          subject: "‚ö†Ô∏è RAG System Drift Detected"
          body: |
            Drift detection alert for Safety Incident Analyzer RAG system.

            Weekly evaluation detected metric regressions.

            Check the GitHub Actions run for details:
            ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          to: ${{ secrets.ALERT_EMAIL_TO }}
          from: RAG Eval Bot
