# Технический разбор системы оценки Safety Incident Analyzer

Этот документ содержит описание архитектуры, концепций и реализации системы оценки (Evaluation Framework) в данном проекте.

---

## 1. Архитектура оценки
Система оценки разделена на три уровня:
1.  **Датасет (`tests/dataset.csv`)**: "Золотой стандарт" вопросов и ответов.
2.  **Движок оценки (`eval/run_full_evaluation.py`)**: Оркестратор, который запускает тесты и собирает результаты.
3.  **Метрики (`src/advanced_generation_metrics.py`)**: Набор LLM-судей и алгоритмов для анализа качества.

---

## 2. Как это реализовано в коде

### А. Загрузка и запуск пайплайна
В файле `eval/run_full_evaluation.py` реализован цикл тестирования:
```python
# Загрузка данных
dataset = load_dataset("tests/dataset.csv")

# Выполнение запроса через всю систему (LangGraph)
response = chain.invoke({"question": question})
answer = response.get("output", "")
context = response.get("context", "")
```
Здесь `chain.invoke` прогоняет вопрос через всех агентов (Relevance, Research, Verification).

### Б. Реализация метрик (LLM-as-a-Judge)
В файле `src/advanced_generation_metrics.py` реализованы "судьи". Каждая метрика — это отдельный запрос к LLM со специальным промптом.

**Пример: Faithfulness (Достоверность)**
```python
prompt = ChatPromptTemplate.from_template(
    "Ты - строгий судья. Оцени, подтверждены ли ВСЕ фактические утверждения "
    "в Ответе предоставленным Контекстом... Верни JSON: {'score': ..., 'reasoning': ...}"
)
# Цепочка: Промпт -> LLM -> Парсер текста
chain = prompt | llm | StrOutputParser()
```
**Идея:** Мы заставляем модель-судью не просто дать цифру, а написать `reasoning` (объяснение). Это резко повышает точность оценки.

### В. Робастный парсинг (Robust JSON Parsing)
Поскольку GigaChat и другие модели часто оборачивают ответы в Markdown, реализована функция `clean_json_response`:
```python
def clean_json_response(text: str) -> str:
    # Извлечение текста между ```json и ``` с помощью регулярных выражений
    match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()
```
Это предотвращает ошибки `JSONDecodeError` и позволяет получать оценки даже если модель "разговорилась".

### Г. Смысловая точность (Correctness)
Реализована в `src/custom_evaluators.py`. Она сравнивает ответ модели с эталоном из CSV.
*   Использует 10-балльную шкалу.
*   Учитывает не только факты, но и полноту/стиль.

---

## 3. Основные метрики и их смысл

| Метка | Реализация | Что проверяет | Если 0.0, то... |
| :--- | :--- | :--- | :--- |
| **Faithfulness** | Сравнение: Ответ vs Контекст | Нет ли галлюцинаций. | Модель придумывает факты, которых нет в документах. |
| **Answer Relevance** | Сравнение: Ответ vs Вопрос | Насколько ответ по существу. | Модель уходит в сторону или отвечает слишком общо. |
| **Correctness** | Сравнение: Ответ vs Эталон | Соответствие правде. | Ошибка в базе знаний или модель сделала неверный вывод. |
| **Citations** | Regex поиск `[cite: X]` | Наличие ссылок на пункты. | Модель не подтверждает свои слова источниками. |

---

## 4. Производительность и P95
В `aggregate_metrics` используется библиотека `numpy`:
```python
metrics["mean_total_time"] = np.mean(all_times)
metrics["p95_total_time"] = np.percentile(all_times, 95)
```
**Зачем P95?** Это критическая метрика для RAG. Если 95% пользователей получают ответ за 12 секунд, а остальные 5% ждут минуту — это проблема, которую не покажет среднее значение (mean).

---

## 5. Как пользоваться этим инструментом
1.  **Отладка промптов:** Посмотри `reasoning` в результатах. Там судья пишет, что именно ему не понравилось.
2.  **A/B тесты:** Запусти оценку, поменяй промпт в `ResearchAgent`, запусти снова. Сравни `mean_correctness_score`.
3.  **Контроль регрессии:** Запускай после каждого крупного изменения, чтобы убедиться, что точность не упала.

---
*Документация создана автоматически для проекта Safety Incident Analyzer.*
