# üìö –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Evaluation —Å–∏—Å—Ç–µ–º—ã

–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —Å eval –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π.

---

## üöÄ –ë–∞–∑–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### 1. –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫: –°–æ–∑–¥–∞–Ω–∏–µ baseline

```bash
# –®–∞–≥ 1: –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É
python eval/run_full_evaluation.py

# –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
cat benchmarks/results_history.jsonl | jq '.'

# –®–∞–≥ 3: –°–æ–∑–¥–∞—Ç—å baseline –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
python -c "
import json

# –ß–∏—Ç–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
with open('benchmarks/results_history.jsonl', 'r') as f:
    lines = f.readlines()
    latest = json.loads(lines[-1])

# –°–æ–∑–¥–∞–µ–º baseline
baseline = {
    'date': latest['timestamp'],
    'version': 'v1.0.0',
    'dataset': latest['dataset'],
    'dataset_size': latest['dataset_size'],
    'metrics': latest['aggregate_metrics']
}

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open('benchmarks/baseline.json', 'w') as f:
    json.dump(baseline, f, indent=2, ensure_ascii=False)

print('‚úÖ Baseline —Å–æ–∑–¥–∞–Ω!')
"
```

---

### 2. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (3-5 –≤–æ–ø—Ä–æ—Å–æ–≤)

```bash
# –î–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
python eval/run_full_evaluation.py --limit 5

# –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞:
# [1/5] –ö—Ç–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç –æ–±—É—á–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–µ –ê?...
#   ‚úÖ Correctness: 8.5/10
#   üìä Faithfulness: 0.92
#   ‚è±Ô∏è  –í—Ä–µ–º—è: 7.8s
```

---

### 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline

```bash
# –ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ eval
python scripts/compare_with_baseline.py

# –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞:
# ======================================================================
# üìä –°–†–ê–í–ù–ï–ù–ò–ï –° BASELINE
# ======================================================================
#
# üìÖ Baseline –¥–∞—Ç–∞: 2025-12-14
# üìÖ –¢–µ–∫—É—â–∏–π –∑–∞–ø—É—Å–∫: 2025-12-15T10:30:00
#
# üéØ –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å: ‚úÖ IMPROVED
#    –£–ª—É—á—à–µ–Ω–∏–π: 2
#    –†–µ–≥—Ä–µ—Å—Å–∏–π: 0
#    –°—Ç–∞–±–∏–ª—å–Ω–æ: 3
#
# ‚úÖ –£–õ–£–ß–®–ï–ù–ò–Ø:
#    ‚Ä¢ Faithfulness
#      Baseline: 0.850
#      –¢–µ–∫—É—â–µ–µ:  0.920
#      –ò–∑–º–µ–Ω–µ–Ω–∏–µ: +0.070 (+8.2%)
```

---

### 4. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

```bash
# –¢–µ—Å—Ç 1: –¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
python eval/run_full_evaluation.py --output benchmarks/test_config_a.jsonl

# –ò–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ config/settings.py
# –ù–∞–ø—Ä–∏–º–µ—Ä: VECTOR_SEARCH_K = 15

# –¢–µ—Å—Ç 2: –ù–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
python eval/run_full_evaluation.py --output benchmarks/test_config_b.jsonl

# –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
python -c "
import json

def load_latest(path):
    with open(path) as f:
        return json.loads(f.readlines()[-1])

config_a = load_latest('benchmarks/test_config_a.jsonl')
config_b = load_latest('benchmarks/test_config_b.jsonl')

print('–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è A:')
print(f\"  Correctness: {config_a['aggregate_metrics']['mean_correctness_score']:.2f}\")
print(f\"  Latency: {config_a['aggregate_metrics']['mean_total_time']:.2f}s\")

print('\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è B:')
print(f\"  Correctness: {config_b['aggregate_metrics']['mean_correctness_score']:.2f}\")
print(f\"  Latency: {config_b['aggregate_metrics']['mean_total_time']:.2f}s\")
"
```

---

## üî¨ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### 5. Eval —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangSmith

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
export LANGSMITH_API_KEY="lsv2_pt_..."
export LANGSMITH_TRACING_V2=true
export LANGSMITH_PROJECT="safety-incident-analyzer"

# –ó–∞–ø—É—Å—Ç–∏—Ç—å eval —á–µ—Ä–µ–∑ LangSmith
python run_ab_test.py

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ LangSmith UI:
# https://smith.langchain.com/
```

---

### 6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤

```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
python scripts/generate_questions.py

# –†–µ–∑—É–ª—å—Ç–∞—Ç: tests/dataset_extended.csv
cat tests/dataset_extended.csv

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
head -n 5 tests/dataset_extended.csv | column -t -s','

# –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
cat tests/dataset.csv > tests/dataset_full.csv
tail -n +2 tests/dataset_extended.csv >> tests/dataset_full.csv

echo "‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–æ $(wc -l < tests/dataset_full.csv) –≤–æ–ø—Ä–æ—Å–æ–≤"
```

---

### 7. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ production –º–µ—Ç—Ä–∏–∫

```python
# –î–æ–±–∞–≤–∏—Ç—å –≤ app.py –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
import json
from datetime import datetime

def log_query_metrics(question, answer, retrieval_time, generation_time):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –≤ production."""
    metrics = {
        "timestamp": datetime.now().isoformat(),
        "question_length": len(question),
        "answer_length": len(answer),
        "retrieval_time": retrieval_time,
        "generation_time": generation_time,
        "total_time": retrieval_time + generation_time,
    }

    with open("production_logs/queries.jsonl", "a") as f:
        f.write(json.dumps(metrics, ensure_ascii=False) + "\n")
```

```bash
# –ê–Ω–∞–ª–∏–∑ production –ª–æ–≥–æ–≤
python -c "
import json
import statistics

with open('production_logs/queries.jsonl') as f:
    metrics = [json.loads(line) for line in f]

latencies = [m['total_time'] for m in metrics]
print(f'–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(metrics)}')
print(f'–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {statistics.mean(latencies):.2f}s')
print(f'P95 –∑–∞–¥–µ—Ä–∂–∫–∞: {statistics.quantiles(latencies, n=20)[18]:.2f}s')
"
```

---

### 8. CI/CD –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

```bash
# –õ–æ–∫–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è CI workflow
act -j evaluate  # –¢—Ä–µ–±—É–µ—Ç 'act' (https://github.com/nektos/act)

# –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Ç–æ, —á—Ç–æ –¥–µ–ª–∞–µ—Ç CI:
python eval/run_full_evaluation.py --limit 5
python scripts/compare_with_baseline.py
```

---

### 9. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö failure cases

```python
# scripts/analyze_failures.py
import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
with open('benchmarks/results_history.jsonl') as f:
    latest = json.loads(f.readlines()[-1])

# –ù–∞—Ö–æ–¥–∏–º –ø–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
failures = []
for result in latest['detailed_results']:
    if result.get('correctness_score', 10) < 6.0:
        failures.append({
            'question': result['question'],
            'answer': result['answer'],
            'ground_truth': result['ground_truth'],
            'correctness': result.get('correctness_score', 0),
            'faithfulness': result.get('faithfulness_score', 0),
        })

print(f"–ù–∞–π–¥–µ–Ω–æ {len(failures)} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤:")
for i, f in enumerate(failures, 1):
    print(f"\n{i}. {f['question'][:60]}...")
    print(f"   Correctness: {f['correctness']:.1f}/10")
    print(f"   Faithfulness: {f['faithfulness']:.2f}")
```

---

### 10. –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

```python
# scripts/export_metrics_csv.py
import json
import csv

# –ß–∏—Ç–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
with open('benchmarks/results_history.jsonl') as f:
    history = [json.loads(line) for line in f]

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ CSV
with open('benchmarks/metrics_timeline.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['timestamp', 'correctness', 'faithfulness', 'latency_p95'])

    for run in history:
        metrics = run['aggregate_metrics']
        writer.writerow([
            run['timestamp'],
            metrics.get('mean_correctness_score', 0),
            metrics.get('mean_faithfulness_score', 0),
            metrics.get('p95_total_time', 0),
        ])

print("‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: benchmarks/metrics_timeline.csv")
```

```bash
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ gnuplot (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
gnuplot <<EOF
set datafile separator ','
set xdata time
set timefmt "%Y-%m-%dT%H:%M:%S"
set format x "%m/%d"
set terminal png size 800,600
set output 'benchmarks/metrics_plot.png'
plot 'benchmarks/metrics_timeline.csv' using 1:2 with lines title 'Correctness'
EOF
```

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ workflow

### Daily Development

```bash
# 1. –í–Ω–µ—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥
# 2. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
python eval/run_full_evaluation.py --limit 5

# 3. –ï—Å–ª–∏ –≤—Å—ë OK - –∫–æ–º–º–∏—Ç
git add .
git commit -m "Improve prompt template"

# 4. CI –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç eval
```

### Weekly Review

```bash
# 1. –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
python eval/run_full_evaluation.py

# 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline
python scripts/compare_with_baseline.py

# 3. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
python scripts/analyze_trends.py  # TODO: —Å–æ–∑–¥–∞—Ç—å

# 4. –û–±–Ω–æ–≤–∏—Ç—å baseline –µ—Å–ª–∏ –µ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è
```

### Before Release

```bash
# 1. –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å—ë–º –¥–∞—Ç–∞—Å–µ—Ç–µ
python eval/run_full_evaluation.py

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
python scripts/check_target_metrics.py  # TODO: —Å–æ–∑–¥–∞—Ç—å

# 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ baseline
cp benchmarks/baseline.json benchmarks/baseline_v1.0.0.json
# –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π baseline –∏–∑ results_history.jsonl

# 4. Tag release
git tag -a v1.1.0 -m "Release v1.1.0 with improved metrics"
```

---

## üìñ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

- [Roadmap –ø—Ä–æ–µ–∫—Ç–∞](../ROADMAP.md)
- [Quick Start](../guides/quick-start.md)
- [Benchmarks README](../../benchmarks/README.md)
