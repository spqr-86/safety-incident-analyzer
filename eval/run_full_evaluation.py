"""
ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° RAG ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹: retrieval + generation + pipeline Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸.

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ:
    python eval/run_full_evaluation.py [--dataset DATASET_PATH] [--output OUTPUT_PATH]
"""

import sys
import json
import csv
import time
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import argparse

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ñ€ÐµÐ½ÑŒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ð² Ð¿ÑƒÑ‚ÑŒ
sys.path.append(str(Path(__file__).parent.parent))

from src.retrieval_metrics import evaluate_retrieval, hit_rate_at_k, mean_reciprocal_rank
from src.advanced_generation_metrics import (
    evaluate_faithfulness,
    evaluate_answer_relevance,
    evaluate_citation_quality,
)
from src.custom_evaluators import check_correctness
from src.llm_factory import get_llm
from src.final_chain import create_final_hybrid_chain


def load_dataset(dataset_path: str) -> List[Dict[str, Any]]:
    """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð¸Ð· CSV."""
    dataset = []
    with open(dataset_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        dataset = list(reader)
    return dataset


def evaluate_single_query(
    question: str,
    ground_truth: str,
    chain,
    retriever,
    llm,
) -> Dict[str, Any]:
    """
    ÐžÑ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ Ð¾Ð´Ð¸Ð½ Ð·Ð°Ð¿Ñ€Ð¾Ñ (retrieval + generation).

    Returns:
        Dict Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸ Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸
    """
    result = {}

    # 1. Retrieval
    retrieval_start = time.time()
    retrieved_docs = retriever.get_relevant_documents(question)
    retrieval_time = time.time() - retrieval_start

    result["retrieval_time"] = retrieval_time
    result["num_retrieved_docs"] = len(retrieved_docs)

    # Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° retrieval (Ð´Ð»Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð½ÑƒÐ¶Ð½Ð° Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚Ð¸)
    result["retrieved_doc_ids"] = [str(i) for i in range(len(retrieved_docs))]

    # 2. Generation
    generation_start = time.time()
    try:
        response = chain.invoke({"question": question})
        answer = response.get("output", "") if isinstance(response, dict) else str(response)
        context = response.get("context", "") if isinstance(response, dict) else ""
    except Exception as e:
        print(f"  âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸: {e}")
        answer = ""
        context = ""

    generation_time = time.time() - generation_start
    total_time = retrieval_time + generation_time

    result["generation_time"] = generation_time
    result["total_time"] = total_time
    result["answer"] = answer
    result["context"] = context

    # 3. Generation Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
    if answer:
        # Faithfulness
        try:
            faith_metrics = evaluate_faithfulness(question, context, answer, llm)
            result.update(faith_metrics)
        except Exception as e:
            print(f"  âš ï¸  Faithfulness eval failed: {e}")
            result["faithfulness_score"] = 0.0

        # Answer relevance
        try:
            rel_metrics = evaluate_answer_relevance(question, answer, llm)
            result.update(rel_metrics)
        except Exception as e:
            print(f"  âš ï¸  Relevance eval failed: {e}")
            result["answer_relevance_score"] = 0.0

        # Citation quality
        cite_metrics = evaluate_citation_quality(answer, context, [])
        result.update(cite_metrics)

        # Correctness (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ evaluator)
        # Ð­Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐµÐ¼ run Ð¸ example Ð´Ð»Ñ check_correctness
        class MockRun:
            def __init__(self, output):
                self.outputs = {"output": output}

        class MockExample:
            def __init__(self, question, ground_truth):
                self.inputs = {"question": question}
                self.outputs = {"ground_truth": ground_truth}

        try:
            correctness = check_correctness(
                MockRun(answer), MockExample(question, ground_truth)
            )
            result["correctness_score"] = correctness.get("score", 0.0)
            result["correctness_comment"] = correctness.get("comment", "")
        except Exception as e:
            print(f"  âš ï¸  Correctness eval failed: {e}")
            result["correctness_score"] = 0.0

    return result


def aggregate_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ÐÐ³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¿Ð¾ Ð²ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼."""
    import numpy as np

    metrics = {}

    # Ð§Ð¸ÑÐ»Ð¾Ð²Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ ÑƒÑÑ€ÐµÐ´Ð½ÐµÐ½Ð¸Ñ
    numeric_keys = [
        "retrieval_time",
        "generation_time",
        "total_time",
        "num_retrieved_docs",
        "faithfulness_score",
        "answer_relevance_score",
        "correctness_score",
        "citation_count",
        "unique_citation_count",
    ]

    for key in numeric_keys:
        values = [r.get(key, 0.0) for r in results if key in r]
        if values:
            metrics[f"mean_{key}"] = float(np.mean(values))
            metrics[f"std_{key}"] = float(np.std(values))
            metrics[f"min_{key}"] = float(np.min(values))
            metrics[f"max_{key}"] = float(np.max(values))
            metrics[f"p95_{key}"] = float(np.percentile(values, 95))

    # Boolean Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
    has_citations_count = sum(1 for r in results if r.get("has_citations", False))
    metrics["citation_rate"] = has_citations_count / len(results) if results else 0.0

    return metrics


def main():
    parser = argparse.ArgumentParser(description="ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° RAG ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹")
    parser.add_argument(
        "--dataset",
        default="tests/dataset.csv",
        help="ÐŸÑƒÑ‚ÑŒ Ðº Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ (CSV)",
    )
    parser.add_argument(
        "--output",
        default="benchmarks/results_history.jsonl",
        help="ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² (JSONL)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ",
    )
    args = parser.parse_args()

    print("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ RAG ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹...")
    print(f"ðŸ“‚ Ð”Ð°Ñ‚Ð°ÑÐµÑ‚: {args.dataset}")

    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    print("\nðŸ”§ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²...")
    llm = get_llm()
    chain, retriever = create_final_hybrid_chain()

    # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°
    dataset = load_dataset(args.dataset)
    if args.limit:
        dataset = dataset[: args.limit]
    print(f"ðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(dataset)} Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²")

    # ÐžÑ†ÐµÐ½ÐºÐ° ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°
    print("\nðŸ“ ÐžÑ†ÐµÐ½ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²...")
    results = []

    for i, item in enumerate(dataset, 1):
        question = item["question"].replace("[cite_start]", "").strip()
        ground_truth = item["ground_truth"]

        print(f"\n[{i}/{len(dataset)}] {question[:60]}...")

        result = evaluate_single_query(question, ground_truth, chain, retriever, llm)
        result["question"] = question
        result["ground_truth"] = ground_truth
        results.append(result)

        # ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð²Ñ‹Ð²Ð¾Ð´
        print(f"  âœ… Correctness: {result.get('correctness_score', 0):.1f}/10")
        print(f"  ðŸ“Š Faithfulness: {result.get('faithfulness_score', 0):.2f}")
        print(f"  â±ï¸  Ð’Ñ€ÐµÐ¼Ñ: {result.get('total_time', 0):.2f}s")

    # ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
    print("\nðŸ“ˆ ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº...")
    agg_metrics = aggregate_metrics(results)

    # Ð’Ñ‹Ð²Ð¾Ð´ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
    print("\n" + "=" * 60)
    print("ðŸ“Š Ð˜Ð¢ÐžÐ“ÐžÐ’Ð«Ð• ÐœÐ•Ð¢Ð Ð˜ÐšÐ˜")
    print("=" * 60)

    print("\nðŸŽ¯ Correctness:")
    print(f"  Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ:     {agg_metrics.get('mean_correctness_score', 0):.2f}/10")
    print(f"  ÐœÐ¸Ð½/ÐœÐ°ÐºÑ:    {agg_metrics.get('min_correctness_score', 0):.1f} / {agg_metrics.get('max_correctness_score', 0):.1f}")

    print("\nâœ… Faithfulness:")
    print(f"  Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ:     {agg_metrics.get('mean_faithfulness_score', 0):.3f}")
    print(f"  ÐœÐ¸Ð½:         {agg_metrics.get('min_faithfulness_score', 0):.3f}")

    print("\nðŸ”— Answer Relevance:")
    print(f"  Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ:     {agg_metrics.get('mean_answer_relevance_score', 0):.3f}")

    print("\nðŸ“š Citations:")
    print(f"  Citation rate:        {agg_metrics.get('citation_rate', 0):.1%}")
    print(f"  Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ð½Ð° Ð¾Ñ‚Ð²ÐµÑ‚:     {agg_metrics.get('mean_citation_count', 0):.1f}")

    print("\nâ±ï¸  Performance:")
    print(f"  Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ:        {agg_metrics.get('mean_total_time', 0):.2f}s")
    print(f"  P95 Ð²Ñ€ÐµÐ¼Ñ:            {agg_metrics.get('p95_total_time', 0):.2f}s")
    print(f"  Retrieval Ð²Ñ€ÐµÐ¼Ñ:      {agg_metrics.get('mean_retrieval_time', 0):.2f}s")
    print(f"  Generation Ð²Ñ€ÐµÐ¼Ñ:     {agg_metrics.get('mean_generation_time', 0):.2f}s")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
    print(f"\nðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² {args.output}...")

    output_data = {
        "timestamp": datetime.now().isoformat(),
        "dataset": args.dataset,
        "dataset_size": len(dataset),
        "aggregate_metrics": agg_metrics,
        "detailed_results": results,
    }

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² JSONL (append)
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(output_data, ensure_ascii=False) + "\n")

    print("âœ… Ð“Ð¾Ñ‚Ð¾Ð²Ð¾!")

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»ÐµÐ²Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº
    print("\nðŸŽ¯ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»ÐµÐ²Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº:")
    checks = [
        ("Correctness > 7.0", agg_metrics.get("mean_correctness_score", 0) > 7.0),
        ("Faithfulness > 0.85", agg_metrics.get("mean_faithfulness_score", 0) > 0.85),
        ("Answer Relevance > 0.80", agg_metrics.get("mean_answer_relevance_score", 0) > 0.80),
        ("P95 Latency < 15s", agg_metrics.get("p95_total_time", 999) < 15.0),
    ]

    for check_name, passed in checks:
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {check_name}")

    if all(passed for _, passed in checks):
        print("\nðŸŽ‰ Ð’ÑÐµ Ñ†ÐµÐ»ÐµÐ²Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð½ÑƒÑ‚Ñ‹!")
    else:
        print("\nâš ï¸  ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ")


if __name__ == "__main__":
    main()
