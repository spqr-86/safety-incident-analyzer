"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç—Ä–∏–∫ —Ü–µ–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/check_target_metrics.py
    python scripts/check_target_metrics.py --results benchmarks/results_history.jsonl
    python scripts/check_target_metrics.py --strict  # Exit code 1 –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∞ —Ü–µ–ª–∏
"""

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, Any, List, Tuple


# –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
TARGET_METRICS = {
    "mean_correctness_score": {
        "target": 8.0,
        "critical": 6.0,
        "direction": "higher",
        "unit": "/10",
        "description": "Correctness score",
    },
    "mean_faithfulness_score": {
        "target": 0.90,
        "critical": 0.70,
        "direction": "higher",
        "unit": "",
        "description": "Faithfulness (no hallucinations)",
    },
    "mean_answer_relevance_score": {
        "target": 0.85,
        "critical": 0.70,
        "direction": "higher",
        "unit": "",
        "description": "Answer relevance",
    },
    "citation_rate": {
        "target": 0.95,
        "critical": 0.80,
        "direction": "higher",
        "unit": "",
        "description": "Citation rate",
    },
    "p95_total_time": {
        "target": 10.0,
        "critical": 20.0,
        "direction": "lower",
        "unit": "s",
        "description": "P95 latency",
    },
}


def load_latest_result(results_path: str) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ JSONL —Ñ–∞–π–ª–∞."""
    results = []
    with open(results_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))

    if not results:
        raise ValueError(f"–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {results_path}")

    return results[-1]


def check_metric(
    metric_key: str,
    current_value: float,
    config: Dict[str, Any],
) -> Tuple[str, str, str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É.

    Returns:
        Tuple[status, emoji, message]
        status: 'achieved' | 'acceptable' | 'critical'
    """
    target = config["target"]
    critical = config["critical"]
    direction = config["direction"]

    if direction == "higher":
        # –•–æ—Ç–∏–º –±–æ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if current_value >= target:
            return "achieved", "‚úÖ", f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ (>= {target})"
        elif current_value >= critical:
            return "acceptable", "‚ö†Ô∏è", f"–ü—Ä–∏–µ–º–ª–µ–º–æ (>= {critical}, —Ü–µ–ª—å: {target})"
        else:
            return "critical", "‚ùå", f"–ö—Ä–∏—Ç–∏—á–Ω–æ (< {critical})"
    else:  # lower
        # –•–æ—Ç–∏–º –º–µ–Ω—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if current_value <= target:
            return "achieved", "‚úÖ", f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ (<= {target})"
        elif current_value <= critical:
            return "acceptable", "‚ö†Ô∏è", f"–ü—Ä–∏–µ–º–ª–µ–º–æ (<= {critical}, —Ü–µ–ª—å: {target})"
        else:
            return "critical", "‚ùå", f"–ö—Ä–∏—Ç–∏—á–Ω–æ (> {critical})"


def check_all_metrics(results: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏.

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    metrics = results.get("aggregate_metrics", {})
    checks = []

    achieved_count = 0
    acceptable_count = 0
    critical_count = 0

    for metric_key, config in TARGET_METRICS.items():
        current_value = metrics.get(metric_key, 0.0)
        status, emoji, message = check_metric(metric_key, current_value, config)

        checks.append({
            "metric": config["description"],
            "key": metric_key,
            "current": current_value,
            "target": config["target"],
            "critical": config["critical"],
            "unit": config["unit"],
            "status": status,
            "emoji": emoji,
            "message": message,
        })

        if status == "achieved":
            achieved_count += 1
        elif status == "acceptable":
            acceptable_count += 1
        else:
            critical_count += 1

    return {
        "timestamp": results.get("timestamp", "unknown"),
        "dataset_size": results.get("dataset_size", 0),
        "checks": checks,
        "summary": {
            "total": len(TARGET_METRICS),
            "achieved": achieved_count,
            "acceptable": acceptable_count,
            "critical": critical_count,
        },
    }


def print_results(check_results: Dict[str, Any], verbose: bool = True) -> None:
    """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏."""
    print("\n" + "=" * 70)
    print("üéØ –ü–†–û–í–ï–†–ö–ê –¶–ï–õ–ï–í–´–• –ú–ï–¢–†–ò–ö")
    print("=" * 70)

    print(f"\nüìÖ Timestamp: {check_results['timestamp']}")
    print(f"üìä Dataset size: {check_results['dataset_size']}")

    summary = check_results["summary"]
    print(f"\nüìà –°–≤–æ–¥–∫–∞:")
    print(f"   ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Ü–µ–ª–µ–≤—ã—Ö:  {summary['achieved']}/{summary['total']}")
    print(f"   ‚ö†Ô∏è  –ü—Ä–∏–µ–º–ª–µ–º–æ:          {summary['acceptable']}/{summary['total']}")
    print(f"   ‚ùå –ö—Ä–∏—Ç–∏—á–Ω—ã—Ö:          {summary['critical']}/{summary['total']}")

    # –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å
    if summary["critical"] > 0:
        overall_status = "‚ùå –ö–†–ò–¢–ò–ß–ù–û"
        overall_color = "\033[91m"  # Red
    elif summary["achieved"] == summary["total"]:
        overall_status = "‚úÖ –í–°–ï –¶–ï–õ–ò –î–û–°–¢–ò–ì–ù–£–¢–´"
        overall_color = "\033[92m"  # Green
    elif summary["acceptable"] + summary["achieved"] == summary["total"]:
        overall_status = "‚ö†Ô∏è –ü–†–ò–ï–ú–õ–ï–ú–û"
        overall_color = "\033[93m"  # Yellow
    else:
        overall_status = "‚ö†Ô∏è –¢–†–ï–ë–£–ï–¢–°–Ø –í–ù–ò–ú–ê–ù–ò–ï"
        overall_color = "\033[93m"  # Yellow

    print(f"\nüéØ –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å: {overall_color}{overall_status}\033[0m")

    # –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ
    if verbose:
        print("\nüìä –î–µ—Ç–∞–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º:")
        print("")

        for check in check_results["checks"]:
            print(f"{check['emoji']} {check['metric']}")
            print(f"   –¢–µ–∫—É—â–µ–µ:  {check['current']:.3f}{check['unit']}")
            print(f"   –¶–µ–ª—å:     {check['target']:.3f}{check['unit']}")
            print(f"   –°—Ç–∞—Ç—É—Å:   {check['message']}")
            print("")

    print("=" * 70)


def save_report(check_results: Dict[str, Any], output_path: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –≤ JSON."""
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(check_results, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ—Ç—Ä–∏–∫ —Ü–µ–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º"
    )
    parser.add_argument(
        "--results",
        default="benchmarks/results_history.jsonl",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏",
    )
    parser.add_argument(
        "--output",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ (JSON)",
    )
    parser.add_argument(
        "--strict",
        action="store_true",
        help="–°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: exit code 1 –µ—Å–ª–∏ –Ω–µ –≤—Å–µ —Ü–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="–ù–µ –≤—ã–≤–æ–¥–∏—Ç—å –¥–µ—Ç–∞–ª–∏, —Ç–æ–ª—å–∫–æ summary",
    )

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    results_path = Path(args.results)
    if not results_path.exists():
        print(f"‚ùå –§–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {results_path}")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python eval/run_full_evaluation.py")
        sys.exit(1)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    results = load_latest_result(str(results_path))

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    check_results = check_all_metrics(results)

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print_results(check_results, verbose=not args.quiet)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    if args.output:
        save_report(check_results, args.output)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if check_results["summary"]["critical"] > 0:
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ eval –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        print("   2. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ failure cases")
        print("   3. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ hyperparameter tuning")
        print("   4. –£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏")

    # Exit code
    if args.strict:
        if check_results["summary"]["achieved"] < check_results["summary"]["total"]:
            print("\n‚ö†Ô∏è –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: –ù–µ –≤—Å–µ —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã")
            sys.exit(1)

    if check_results["summary"]["critical"] > 0:
        print("\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏!")
        sys.exit(1)

    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞!")
    sys.exit(0)


if __name__ == "__main__":
    main()
